{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Part3_1ValueIteration.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boernd/rl-workshop/blob/main/Kopie_von_Part3_1ValueIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gulWAmO35A4",
        "outputId": "8c9531ba-fe63-4b12-8b53-f21f714eb7ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 11.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raItN8xD3vLq",
        "outputId": "b6d5031a-10e6-4c35-85fa-fe4e6396da5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n",
        "# In the beginning, we import used packages and define constants:\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "# Then we define the Agent class, which will keep our tables and contain functions we'll be\n",
        "# using in the training loop:\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "# In the class constructor, we create the environment we'll be using for data samples, obtain\n",
        "# our first observation, and define tables for rewards, transitions, and values.\n",
        "\n",
        "# This function \"play_n_random_steps\" is used to gather random experience from the environment and update\n",
        "# reward and transition tables. Note that we don't need to wait for the end of the episode to\n",
        "# start learning; we just perform N steps and remember their outcomes. This is one of the\n",
        "# differences between Value iteration and Cross-entropy, which can learn only on full\n",
        "# episodes.\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = self.env.reset() if is_done else new_state\n",
        "\n",
        "#The next function calculates the value of the action from the state, using our transition,\n",
        "# reward and values tables. We will use it for two purposes: to select the best action to\n",
        "# perform from the state and to calculate the new value of the state on value iteration. Its\n",
        "# logic is illustrated in the following diagram and we do the following:\n",
        "# 1. We extract transition counters for the given state and action from the transition\n",
        "# table. Counters in this table have a form of dict, with target states as key and a\n",
        "# count of experienced transitions as value. We sum all counters to obtain the total\n",
        "# count of times we've executed the action from the state. We will use this total\n",
        "# value later to go from an individual counter to probability.\n",
        "# 2. Then we iterate every target state that our action has landed on and calculate its\n",
        "# contribution into the total action value using the Bellman equation. This\n",
        "# contribution equals to immediate reward plus discounted value for the target\n",
        "# state. We multiply this sum to the probability of this transition and add the result to\n",
        "# the final action value. \n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for tgt_state, count in target_counts.items():\n",
        "            reward = self.rewards[(state, action, tgt_state)]\n",
        "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
        "        return action_value\n",
        "\n",
        "# The next function \"select_action\" uses the function we just described to make a decision about the best\n",
        "# action to take from the given state. It iterates over all possible actions in the environment\n",
        "# and calculates value for every action. The action with the largest value wins and is returned\n",
        "# as the action to take. This action selection process is deterministic, as\n",
        "# the play_n_random_steps() function introduces enough exploration. So, our agent will\n",
        "# behave greedily in regard to our value approximation.\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "# The \"play_episode function\" uses select_action to find the best action to take and plays one\n",
        "# full episode using the provided environment. This function is used to play test episodes,\n",
        "# during which we don't want to mess up with the current state of the main environment\n",
        "# used to gather random data. So, we're using the second environment passed as an\n",
        "# argument. The logic is very simple and should be already familiar to you: we just loop over\n",
        "# states accumulating reward for one episode:\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "# The final method of the Agent class is our \"value iteration\" implementation and it is\n",
        "# surprisingly simple, thanks to the preceding functions. What we do is just loop over all\n",
        "# states in the environment, then for every state we calculate the values for the states\n",
        "# reachable from it, obtaining candidates for the value of the state. Then we update the value\n",
        "# of our current state with the maximum value of the action available from the state:\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [self.calc_action_value(state, action)\n",
        "                            for action in range(self.env.action_space.n)]\n",
        "            self.values[state] = max(state_values)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_env = gym.make(ENV_NAME)\n",
        "    agent = Agent()\n",
        "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
        "\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        agent.play_n_random_steps(100)\n",
        "        agent.value_iteration()\n",
        "\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        writer.add_scalar(\"reward\", reward, iter_no)\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "    writer.close()\n",
        "\n",
        "# Our solution is stochastic, and my experiments usually required from 12 to 100 iterations to\n",
        "# reach a solution, but in all cases, it took less than a second to find a good policy that could\n",
        "# solve the environment in 80% of runs. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.000 -> 0.050\n",
            "Best reward updated 0.050 -> 0.150\n",
            "Best reward updated 0.150 -> 0.300\n",
            "Best reward updated 0.300 -> 0.400\n",
            "Best reward updated 0.400 -> 0.650\n",
            "Best reward updated 0.650 -> 0.700\n",
            "Best reward updated 0.700 -> 0.750\n",
            "Best reward updated 0.750 -> 0.800\n",
            "Best reward updated 0.800 -> 0.850\n",
            "Solved in 44 iterations!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}