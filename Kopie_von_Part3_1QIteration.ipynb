{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Part3_1QIteration.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boernd/rl-workshop/blob/main/Kopie_von_Part3_1QIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZknuCofm5Mek"
      },
      "source": [
        "# Q iteration in practice\n",
        "The central data\n",
        "structures in this example are as follows:\n",
        "\n",
        "• Reward table: A dictionary with the composite key \"source state\" + \"action\" +\n",
        "\"target state\". The value is obtained from the immediate reward.\n",
        "\n",
        "• Transitions table: A dictionary keeping counters of the experienced transitions.\n",
        "The key is the composite \"state\" + \"action\" and the value is another dictionary that maps the target state into a count of times that we've seen it. For example, if in state 0 we execute action 1 ten times, after three times it leads us to state 4 and after seven times to state 5. Entry with the key (0, 1) in this table will be a dict {4:\n",
        "3, 5: 7}. We use this table to estimate the probabilities of our transitions.\n",
        "\n",
        "• Value table: A dictionary that maps a state into the calculated value of this state.\n",
        "\n",
        "**The overall logic of our code is simple:**\n",
        " in the loop, we play 100 random steps from the\n",
        "environment, populating the reward and transition tables. After those 100 steps, we\n",
        "perform a value iteration loop over all states, updating our value table. Then we play\n",
        "several full episodes to check our improvements using the updated value table. If the\n",
        "average reward for those test episodes is above the **0.8** boundary , then we stop training.\n",
        "During test episodes, we also update our reward and transition tables to use all data from\n",
        "the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFtRzi6h4W9N",
        "outputId": "bb24ac8e-5ae1-4375-ba63-651527198dd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkWmPECp4Ko7",
        "outputId": "86922cd3-37d3-4620-9d38-66d4db1d5d48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n",
        "# In the beginning, we import used packages and define constants:\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "# Then we define the Agent class, which will keep our tables and contain functions we'll be\n",
        "# using in the training loop:\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "# In the class constructor, we create the environment we'll be using for data samples, obtain\n",
        "# our first observation, and define tables for rewards, transitions, and values.\n",
        "\n",
        "# This function \"play_n_random_steps\" is used to gather random experience from the environment and update\n",
        "# reward and transition tables. Note that we don't need to wait for the end of the episode to\n",
        "# start learning; we just perform N steps and remember their outcomes. This is one of the\n",
        "# differences between Value iteration and Cross-entropy, which can learn only on full\n",
        "# episodes.\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = self.env.reset() if is_done else new_state\n",
        "\n",
        "# The next function \"select_action\" uses the function we just described to make a decision about the best\n",
        "# action to take from the given state. It iterates over all possible actions in the environment\n",
        "# and calculates value for every action. The action with the largest value wins and is returned\n",
        "# as the action to take. This action selection process is deterministic, as\n",
        "# the play_n_random_steps() function introduces enough exploration. So, our agent will\n",
        "# behave greedily in regard to our value approximation.\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "# The \"play_episode function\" uses select_action to find the best action to take and plays one\n",
        "# full episode using the provided environment. This function is used to play test episodes,\n",
        "# during which we don't want to mess up with the current state of the main environment\n",
        "# used to gather random data. So, we're using the second environment passed as an\n",
        "# argument. The logic is very simple and should be already familiar to you: we just loop over\n",
        "# states accumulating reward for one episode:\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "# The code \"value_iteration\" is very similar to calc_action_value in the previous example and in fact it does\n",
        "# almost the same thing. For the given state and action, it needs to calculate the value of this\n",
        "# action using statistics about target states that we've reached with the action. To calculate\n",
        "# this value, we use the Bellman equation and our counters, which allow us to approximate\n",
        "# the probability of the target state. However, in Bellman's equation we have the value of the\n",
        "# state and now we need to calculate it differently. Before, we had it stored in the value table\n",
        "# (as we approximated the value of states), so we just took it from this table. We can't do this\n",
        "# anymore, so we have to call the select_action method, which will choose for us the action \n",
        "# with the largest Q-value, and then we take this Q-value as the value of the target state. Of\n",
        "# course, we can implement another function which could calculate for us this value of state,\n",
        "# but select_action does almost everything we need, so we will reuse it here.\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            for action in range(self.env.action_space.n):\n",
        "                action_value = 0.0\n",
        "                target_counts = self.transits[(state, action)]\n",
        "                total = sum(target_counts.values())\n",
        "                for tgt_state, count in target_counts.items():\n",
        "                    reward = self.rewards[(state, action, tgt_state)]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
        "                self.values[(state, action)] = action_value\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_env = gym.make(ENV_NAME)\n",
        "    agent = Agent()\n",
        "    writer = SummaryWriter(comment=\"-q-iteration\")\n",
        "\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        agent.play_n_random_steps(100)\n",
        "        agent.value_iteration()\n",
        "\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        writer.add_scalar(\"reward\", reward, iter_no)\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "    writer.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.000 -> 0.250\n",
            "Best reward updated 0.250 -> 0.550\n",
            "Best reward updated 0.550 -> 0.600\n",
            "Best reward updated 0.600 -> 0.700\n",
            "Best reward updated 0.700 -> 0.850\n",
            "Solved in 19 iterations!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}